"""
è¯­éŸ³è¯†åˆ«å’Œè¯´è¯äººåˆ†ç¦»æ¨¡å—
ä½¿ç”¨Whisperè¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼Œpyannote.audioè¿›è¡Œè¯´è¯äººåˆ†ç¦»
"""

import os
import json
import torch
from typing import List, Dict
from tqdm import tqdm
import whisper
from pyannote.audio import Pipeline
from dotenv import load_dotenv

load_dotenv()


class Transcriber:
    """è¯­éŸ³è¯†åˆ«å™¨"""

    def __init__(self, model_size: str = "medium", device: str = None):
        """
        åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«å™¨

        Args:
            model_size: Whisperæ¨¡å‹å¤§å° (tiny, base, small, medium, large)
            device: ä½¿ç”¨çš„è®¾å¤‡ (cuda/cpu)
        """
        self.model_size = model_size
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")

        print(f"ğŸ“ åŠ è½½Whisperæ¨¡å‹ ({model_size})...")
        self.model = whisper.load_model(model_size, device=self.device)
        print(f"âœ“ Whisperæ¨¡å‹åŠ è½½å®Œæˆ (è®¾å¤‡: {self.device})")

    def transcribe(self, audio_path: str) -> List[Dict]:
        """
        è¯­éŸ³è¯†åˆ«

        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„

        Returns:
            è¯†åˆ«ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«start, end, text
        """
        print(f"ğŸ“ æ­£åœ¨è¿›è¡Œè¯­éŸ³è¯†åˆ«...")
        result = self.model.transcribe(
            audio_path,
            language=None,  # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
            task="transcribe",
            word_timestamps=True,  # è·å–è¯çº§æ—¶é—´æˆ³
        )

        segments = []
        for seg in result['segments']:
            segments.append({
                'start': seg['start'],
                'end': seg['end'],
                'text': seg['text'].strip(),
            })

        print(f"âœ“ è¯†åˆ«å®Œæˆï¼Œå…±{len(segments)}ä¸ªç‰‡æ®µ")
        return segments


class SpeakerDiarization:
    """è¯´è¯äººåˆ†ç¦»å™¨"""

    def __init__(self, hf_token: str = None):
        """
        åˆå§‹åŒ–è¯´è¯äººåˆ†ç¦»å™¨

        Args:
            hf_token: Hugging Face token
        """
        self.hf_token = hf_token or os.getenv('HF_TOKEN')

        if not self.hf_token:
            raise ValueError(
                "éœ€è¦Hugging Face tokenã€‚è¯·è®¾ç½®HF_TOKENç¯å¢ƒå˜é‡æˆ–ä¼ å…¥hf_tokenå‚æ•°ã€‚\n"
                "1. æ³¨å†ŒHugging Faceè´¦å·: https://huggingface.co/join\n"
                "2. ç”Ÿæˆtoken: https://huggingface.co/settings/tokens\n"
                "3. æ¥å—ç”¨æˆ·åè®®: https://huggingface.co/pyannote/speaker-diarization-3.1"
            )

        print("ğŸ“ åŠ è½½è¯´è¯äººåˆ†ç¦»æ¨¡å‹...")
        self.pipeline = Pipeline.from_pretrained(
            "pyannote/speaker-diarization-3.1",
            use_auth_token=self.hf_token,
        )

        # å°†pipelineç§»åŠ¨åˆ°GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.pipeline = self.pipeline.to(device)
        print(f"âœ“ è¯´è¯äººåˆ†ç¦»æ¨¡å‹åŠ è½½å®Œæˆ (è®¾å¤‡: {device})")

    def diarize(self, audio_path: str) -> List[Dict]:
        """
        è¯´è¯äººåˆ†ç¦»

        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„

        Returns:
            è¯´è¯äººç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«start, end, speaker
        """
        print("ğŸ“ æ­£åœ¨è¿›è¡Œè¯´è¯äººåˆ†ç¦»...")
        diarization = self.pipeline(audio_path)

        speakers = []
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            speakers.append({
                'start': float(turn.start),
                'end': float(turn.end),
                'speaker': speaker,
            })

        print(f"âœ“ è¯´è¯äººåˆ†ç¦»å®Œæˆï¼Œå…±{len(speakers)}ä¸ªç‰‡æ®µ")
        return speakers


class TranscriberWithSpeaker:
    """è¯­éŸ³è¯†åˆ« + è¯´è¯äººåˆ†ç¦»"""

    def __init__(self, model_size: str = "medium", hf_token: str = None):
        """
        åˆå§‹åŒ–

        Args:
            model_size: Whisperæ¨¡å‹å¤§å°
            hf_token: Hugging Face token
        """
        self.transcriber = Transcriber(model_size=model_size)
        self.diarization = SpeakerDiarization(hf_token=hf_token)

    def process(self, audio_path: str) -> List[Dict]:
        """
        å¤„ç†éŸ³é¢‘ï¼Œè¿”å›å¸¦è¯´è¯äººæ ‡ç­¾çš„æ–‡å­—ç¨¿

        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„

        Returns:
            æ–‡å­—ç¨¿åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«start, end, speaker, text
        """
        # è¯­éŸ³è¯†åˆ«
        transcription = self.transcriber.transcribe(audio_path)

        # è¯´è¯äººåˆ†ç¦»
        speakers = self.diarization.diarize(audio_path)

        # åˆå¹¶ç»“æœ
        result = []
        for seg in tqdm(transcription, desc="åˆå¹¶è¯†åˆ«ç»“æœ"):
            # æ‰¾åˆ°ä¸å½“å‰æ–‡å­—ç‰‡æ®µé‡å æœ€å¤šçš„è¯´è¯äººç‰‡æ®µ
            best_speaker = None
            max_overlap = 0

            for speaker in speakers:
                # è®¡ç®—é‡å æ—¶é—´
                overlap_start = max(seg['start'], speaker['start'])
                overlap_end = min(seg['end'], speaker['end'])
                overlap = max(0, overlap_end - overlap_start)

                if overlap > max_overlap:
                    max_overlap = overlap
                    best_speaker = speaker['speaker']

            result.append({
                'start': seg['start'],
                'end': seg['end'],
                'speaker': best_speaker or 'UNKNOWN',
                'text': seg['text'],
            })

        return result

    def save_result(self, result: List[Dict], output_path: str):
        """
        ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶

        Args:
            result: å¤„ç†ç»“æœ
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"âœ“ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")

    def load_result(self, input_path: str) -> List[Dict]:
        """
        ä»JSONæ–‡ä»¶åŠ è½½ç»“æœ

        Args:
            input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„

        Returns:
            å¤„ç†ç»“æœ
        """
        with open(input_path, 'r', encoding='utf-8') as f:
            return json.load(f)
